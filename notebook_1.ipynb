{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### For Sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxYm_oJZncKh",
        "outputId": "07060908-3627-4f7a-c337-8b6abbfd81f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-97wybq5niyN",
        "outputId": "4aab30bd-48a8-4e8c-e505-807637e01834"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZsDZwDuTnkYj"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained Word2Vec model\n",
        "model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# Load stopwords from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BkzbgE3Wnl2r"
      },
      "outputs": [],
      "source": [
        "# Function for text cleaning and preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove non-alphanumeric characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize text\n",
        "    words = nltk.word_tokenize(text)\n",
        "    return words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6ai7SN6Xnlxb"
      },
      "outputs": [],
      "source": [
        "# Function to find semantically similar words\n",
        "def find_similar_words(word, topn=5):\n",
        "    if word in model.key_to_index:\n",
        "        similar_words = model.most_similar(word, topn=topn)\n",
        "        return similar_words\n",
        "    else:\n",
        "        return f\"Word '{word}' not in vocabulary\"\n",
        "\n",
        "# Function to compute semantic distance (cosine similarity)\n",
        "def semantic_distance(word1, word2):\n",
        "    if word1 in model.key_to_index and word2 in model.key_to_index:\n",
        "        similarity = model.similarity(word1, word2)\n",
        "        return similarity\n",
        "    else:\n",
        "        return f\"One or both words '{word1}' and '{word2}' not in vocabulary\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RbEvChL2nlus"
      },
      "outputs": [],
      "source": [
        "# Function to transform the sentence\n",
        "def transform_sentence(sentence, rank=1):\n",
        "    words = preprocess_text(sentence)\n",
        "    transformed_words = []\n",
        "    for word in words:\n",
        "        if word not in stop_words:\n",
        "            similar_words = find_similar_words(word, topn=rank)\n",
        "            if isinstance(similar_words, str):\n",
        "                transformed_words.append(word)\n",
        "            else:\n",
        "                similar_word, similarity = similar_words[rank-1]  # Get the N-th similar word\n",
        "                transformed_words.append(similar_word)\n",
        "        else:\n",
        "            transformed_words.append(word)\n",
        "    return ' '.join(transformed_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufw5v5epnlot",
        "outputId": "7a85fa76-8949-4abe-b613-6d39a0d2e86d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similar words for 'president': ['President', 'chairman', 'vice_president', 'chief_executive', 'CEO']\n",
            "Similar words for 'greet': ['greets', 'greeting', 'Greet', 'greeted', 'warmly_greeted']\n",
            "Similar words for 'press': ['media', 'reporters', 'hastily_convened_press', 'breifing', 'news']\n",
            "Similar words for 'chicago': ['baltimore', 'denver', 'nyc', 'atlanta', 'springfield']\n",
            "Original sentence: President greet the press in chicago\n",
            "Transformed sentence: chairman greeting the reporters in denver\n"
          ]
        }
      ],
      "source": [
        "# Input sentence and rank\n",
        "input_sentence = \"President greet the press in chicago\"\n",
        "rank = 2  # we can change this to 1st similar, 2nd similar, etc.\n",
        "\n",
        "# Display semantically similar words for each word in the sentence\n",
        "words = preprocess_text(input_sentence)\n",
        "for word in words:\n",
        "    if word not in stop_words:\n",
        "        similar_words = find_similar_words(word, topn=5)\n",
        "        if isinstance(similar_words, str):\n",
        "            print(similar_words)\n",
        "        else:\n",
        "            print(f\"Similar words for '{word}': {[w for w, _ in similar_words]}\")\n",
        "\n",
        "# Transform the sentence based on semantic distance\n",
        "transformed_sentence = transform_sentence(input_sentence, rank)\n",
        "\n",
        "# Display result\n",
        "print(f\"Original sentence: {input_sentence}\")\n",
        "print(f\"Transformed sentence: {transformed_sentence}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### New Method for Anonymizing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: numpy in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (1.26.0)\n",
            "Requirement already satisfied: gensim in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (4.3.3)\n",
            "Requirement already satisfied: torch in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (2.6.0+cu118)\n",
            "Requirement already satisfied: transformers in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (4.49.0)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: setuptools in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from torch) (74.0.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.29.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2024.7.24)\n",
            "Requirement already satisfied: requests in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: wrapt in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\2756012\\appdata\\roaming\\python\\python312\\site-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install numpy gensim torch transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "HTTPError",
          "evalue": "HTTP Error 403: Forbidden",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mapi\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load GloVe embeddings\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m glove_model \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mglove-wiki-gigaword-100\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# You can choose a different model if needed\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_embedding\u001b[39m(word):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m glove_model:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gensim\\downloader.py:496\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    494\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_dir, file_name)\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(folder_dir):\n\u001b[1;32m--> 496\u001b[0m     \u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_path:\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\gensim\\downloader.py:365\u001b[0m, in \u001b[0;36m_download\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    363\u001b[0m tmp_dir \u001b[38;5;241m=\u001b[39m tempfile\u001b[38;5;241m.\u001b[39mmkdtemp()\n\u001b[0;32m    364\u001b[0m init_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tmp_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__init__.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 365\u001b[0m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_load_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    366\u001b[0m total_parts \u001b[38;5;241m=\u001b[39m _get_parts(name)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_parts \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\urllib\\request.py:240\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;124;03mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m url_type, path \u001b[38;5;241m=\u001b[39m _splittype(url)\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mclosing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m    241\u001b[0m     headers \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39minfo()\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\urllib\\request.py:215\u001b[0m, in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[1;32m--> 215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\urllib\\request.py:521\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[0;32m    520\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[1;32m--> 521\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\urllib\\request.py:630\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[1;32m--> 630\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
            "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\urllib\\request.py:559\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[0;32m    558\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\urllib\\request.py:492\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    491\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 492\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[1;32mc:\\Program Files\\Python312\\Lib\\urllib\\request.py:639\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[1;32m--> 639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
            "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 403: Forbidden"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "\n",
        "# Load GloVe embeddings\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\")  # You can choose a different model if needed\n",
        "\n",
        "def get_embedding(word):\n",
        "    if word in glove_model:\n",
        "        return glove_model[word]\n",
        "    else:\n",
        "        return None  # Return None if the word is not in the vocabulary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def anonymize_numerical(value, epsilon=1.0):\n",
        "    noise = np.random.laplace(0, 1/epsilon)\n",
        "    return value + noise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def find_similar_word(word, distance_threshold=0.10):\n",
        "    word_embedding = get_embedding(word)\n",
        "    if word_embedding is None:\n",
        "        return word  # Return original word if not found\n",
        "\n",
        "    # Prepare to find similar words\n",
        "    similar_words = []\n",
        "    \n",
        "    for candidate in glove_model.key_to_index.keys():\n",
        "        candidate_embedding = get_embedding(candidate)\n",
        "        if candidate_embedding is not None:\n",
        "            similarity = cosine_similarity([word_embedding], [candidate_embedding])[0][0]\n",
        "            distance = 1 - similarity  # Convert similarity to distance\n",
        "            if distance >= distance_threshold:\n",
        "                similar_words.append((candidate, distance))\n",
        "\n",
        "    # Sort by distance and return the first similar word if available\n",
        "    similar_words.sort(key=lambda x: x[1])  # Sort by distance\n",
        "    return similar_words[0][0] if similar_words else word  # Return the first similar word or original\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def anonymize_string(input_string, distance=0.10):\n",
        "    try:\n",
        "        # Check if the input is numerical\n",
        "        value = float(input_string)\n",
        "        return anonymize_numerical(value)\n",
        "    except ValueError:\n",
        "        # If not numerical, proceed with string anonymization\n",
        "        return find_similar_word(input_string, distance)\n",
        "\n",
        "# Example usage\n",
        "print(anonymize_string(\"42\"))  # Numerical input\n",
        "print(anonymize_string(\"example\"))  # String input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Code to be added in main masking model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load GloVe embeddings\n",
        "glove_model = api.load(\"glove-wiki-gigaword-100\")  # You can choose a different model if needed\n",
        "\n",
        "def get_embedding(word):\n",
        "    if word in glove_model:\n",
        "        return glove_model[word]\n",
        "    else:\n",
        "        return None  # Return None if the word is not in the vocabulary\n",
        "\n",
        "def anonymize_numerical(value, epsilon=1.0):\n",
        "    noise = np.random.laplace(0, 1/epsilon)\n",
        "    return value + noise\n",
        "\n",
        "def find_similar_word(word, distance_threshold=0.10):\n",
        "    word_embedding = get_embedding(word)\n",
        "    if word_embedding is None:\n",
        "        return word  # Return original word if not found\n",
        "\n",
        "    similar_words = []\n",
        "    \n",
        "    for candidate in glove_model.key_to_index.keys():\n",
        "        candidate_embedding = get_embedding(candidate)\n",
        "        if candidate_embedding is not None:\n",
        "            similarity = cosine_similarity([word_embedding], [candidate_embedding])[0][0]\n",
        "            distance = 1 - similarity  # Convert similarity to distance\n",
        "            if distance >= distance_threshold:\n",
        "                similar_words.append((candidate, distance))\n",
        "\n",
        "    similar_words.sort(key=lambda x: x[1])  # Sort by distance\n",
        "    return similar_words[0][0] if similar_words else word  # Return the first similar word or original\n",
        "\n",
        "def anonymize_pii1(text_input: str, result: List[RecognizerResult], attributedb: AttributeDB, cat: str) -> str:\n",
        "    \"\"\"\n",
        "    Mask the text PIIs according to disclosure proportion:\n",
        "    - If the entity is present in the category (CATEGORY_ENTITIES), apply its defined masking method.\n",
        "    - If the entity is not in the category, completely mask it.\n",
        "    \"\"\"\n",
        "    # Get the list of allowed entities for the given category\n",
        "    category_entities = dcfg.CATEGORY_ENTITIES.get(cat, [])\n",
        "    operator_options = {}\n",
        "\n",
        "    for entity in dcfg.ENTITIES:\n",
        "        if entity in category_entities:\n",
        "            # Apply category-specific masking plan\n",
        "            operator_options[entity] = OperatorConfig(\n",
        "                \"custom\", {\"lambda\": attributedb.get_attr_obj_from_type(entity).masking_plan}\n",
        "            )\n",
        "        else:\n",
        "            # Completely mask entities not in the category with semantic or numerical anonymization\n",
        "            # Here we will replace the entity with its anonymized version\n",
        "            operator_options[entity] = OperatorConfig(\n",
        "                \"replace\", {\"new_value\": anonymize_entity(text_input, entity)}\n",
        "            )\n",
        "\n",
        "    # Anonymize the input text based on PII detection\n",
        "    anonymizer = AnonymizerEngine()\n",
        "    anonymized_text = anonymizer.anonymize(\n",
        "        text=text_input,\n",
        "        analyzer_results=result,  # type: ignore\n",
        "        operators=operator_options\n",
        "    ).text\n",
        "\n",
        "    return anonymized_text\n",
        "\n",
        "def anonymize_entity(text_input: str, entity: str) -> str:\n",
        "    \"\"\"\n",
        "    Anonymize the entity based on whether it's numerical or a regular string.\n",
        "    \"\"\"\n",
        "    # Check if the entity is numerical\n",
        "    try:\n",
        "        value = float(entity)\n",
        "        return str(anonymize_numerical(value))  # Anonymize numerically\n",
        "    except ValueError:\n",
        "        # If not numerical, proceed with string anonymization\n",
        "        return find_similar_word(entity)  # Anonymize semantically\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
